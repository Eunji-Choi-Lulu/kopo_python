# -*- coding: utf-8 -*-
"""0417_다국어 번역 품질평가 관계데이터 수집.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1--q2QwH1h-zN5PAQSDlQMmYhDMBvoVQ5

# 1. JSON형태의 문서대상 기계 독해 데이터 (출처: AIHub → "다국어 번역 품질 평가 데이터")
"""

# 'datasets' 라이브러리의 2.21.0 버전을 설치하는 명령어
!pip install datasets==2.21.0

#requests, json, pandas 라이브러리 불러오기
#Hugging Face에 연결해 데이터셋 업로드 할 수 있는 라이브러리 불러오기
import requests
import json
import pandas as pd
from datasets import Dataset
import huggingface_hub

#url 불러오기
url = "https://raw.githubusercontent.com/Eunji-Choi-Lulu/enko_json/refs/heads/main/enko_%EC%82%AC%ED%9A%8C%EA%B3%BC%ED%95%99_train_13330.json"

#예외처리시도 + 타겟 URL에 데이터 요청
try:
    response = requests.get(url=url)
except Exception as e:
    print(e)

#response.text

#응답받은 JSON 텍스트 데이터를 Python의 딕셔너리 형태로 변환
original_data = json.loads(response.text)
print(original_data)

#원본 데이터의 키 확인
original_data.keys()

#original_data["data"]의 항목 개수 확인
len(original_data["data"])

#"data"키에 해당하는 실제 데이터 리스트 datas변수에 저장
datas = original_data["data"]

"""##Alpaca 포멧(instruction, input, output) 형태로 수집"""

#데이터 하나만 뽑아서 실제 데이터 구조 확인해보기
eachData = datas[0]
eachData

#하나의 데이터 셋 내 주요 key 확인
eachData.keys()

#각 컬럼 리스트 초기화
instruction = []
input = []
output = []

#각 데이터를 순회
for data in datas:
    instruction.append(data["SN"])
    input.append(data["source_cleaned"])
    output.append(data["ht"])

finalDf = pd.DataFrame(zip(instruction, input, output))
finalDf.columns = ["instruction", "input", "output"]
finalDf

huggingface_hub.login("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")

dataset = Dataset.from_pandas(finalDf)

dataset.push_to_hub("EunjiChoi/enkoDF")